{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T07:15:49.015087Z","iopub.execute_input":"2021-11-16T07:15:49.016819Z","iopub.status.idle":"2021-11-16T07:15:49.032544Z","shell.execute_reply.started":"2021-11-16T07:15:49.016762Z","shell.execute_reply":"2021-11-16T07:15:49.031847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Nama: Muh. Muslim Al Mujahid\n### NIM: 13518054","metadata":{}},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:49.034501Z","iopub.execute_input":"2021-11-16T07:15:49.034812Z","iopub.status.idle":"2021-11-16T07:15:49.040917Z","shell.execute_reply.started":"2021-11-16T07:15:49.034777Z","shell.execute_reply":"2021-11-16T07:15:49.040232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertLMHeadModel, BertConfig, BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:49.042204Z","iopub.execute_input":"2021-11-16T07:15:49.043175Z","iopub.status.idle":"2021-11-16T07:15:49.054697Z","shell.execute_reply.started":"2021-11-16T07:15:49.043126Z","shell.execute_reply":"2021-11-16T07:15:49.053744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:49.056279Z","iopub.execute_input":"2021-11-16T07:15:49.056773Z","iopub.status.idle":"2021-11-16T07:15:49.07123Z","shell.execute_reply.started":"2021-11-16T07:15:49.056737Z","shell.execute_reply":"2021-11-16T07:15:49.070002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/squad-v11/SQuAD-v1.1.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:49.073301Z","iopub.execute_input":"2021-11-16T07:15:49.073564Z","iopub.status.idle":"2021-11-16T07:15:50.672687Z","shell.execute_reply.started":"2021-11-16T07:15:49.073535Z","shell.execute_reply":"2021-11-16T07:15:50.671556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()\ndf.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:50.674119Z","iopub.execute_input":"2021-11-16T07:15:50.674339Z","iopub.status.idle":"2021-11-16T07:15:50.733638Z","shell.execute_reply.started":"2021-11-16T07:15:50.674311Z","shell.execute_reply":"2021-11-16T07:15:50.732612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = df.drop(columns=['title', 'answer_start', 'answer_end'])\ndataset = pd.DataFrame({ \n    'text': '<answer> ' + dataset['answer'] + ' <context> ' + dataset['context'],\n    'question': dataset['question']\n})\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:50.735029Z","iopub.execute_input":"2021-11-16T07:15:50.735267Z","iopub.status.idle":"2021-11-16T07:15:50.827918Z","shell.execute_reply.started":"2021-11-16T07:15:50.735238Z","shell.execute_reply":"2021-11-16T07:15:50.827007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, validation_data = train_test_split(dataset, test_size=0.15, random_state=42)\nprint('train: %s' % len(train_data))\nprint('validation: %s' % len(validation_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:50.832066Z","iopub.execute_input":"2021-11-16T07:15:50.832413Z","iopub.status.idle":"2021-11-16T07:15:50.841518Z","shell.execute_reply.started":"2021-11-16T07:15:50.832373Z","shell.execute_reply":"2021-11-16T07:15:50.840635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset","metadata":{}},{"cell_type":"code","source":"PRETRAINED_MODEL = 'bert-base-uncased'\nDIR = \"kaggle/working\"\nBATCH_SIZE = 4\nSEQ_LENGTH = 512\n\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\ntokenizer.add_special_tokens(\n    {'additional_special_tokens': ['<answer>', '<context>']}\n)\n\nclass QGDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n         return len(self.df)\n\n    def __getitem__(self, idx):   \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        row = self.df.iloc[idx]       \n\n        encoded_text = tokenizer(\n            text=row['text'],\n            padding='max_length', \n            max_length=SEQ_LENGTH,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        encoded_text['input_ids'] = torch.squeeze(encoded_text['input_ids'])\n        encoded_text['attention_mask'] = torch.squeeze(encoded_text['attention_mask'])\n\n        encoded_question = tokenizer(\n            text=row['question'],\n            padding='max_length',\n            max_length=SEQ_LENGTH,\n            truncation=True,\n            return_tensors='pt'\n        )\n        encoded_question['input_ids'] = torch.squeeze(encoded_question['input_ids'])\n\n        return (encoded_text.to(device), encoded_question.to(device))\n\ntrain_set = QGDataset(train_data)\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\nvalid_set = QGDataset(validation_data) \nvalid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:50.84295Z","iopub.execute_input":"2021-11-16T07:15:50.843209Z","iopub.status.idle":"2021-11-16T07:15:53.527112Z","shell.execute_reply.started":"2021-11-16T07:15:50.843178Z","shell.execute_reply":"2021-11-16T07:15:53.526423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"LR = 0.001\nEPOCHS = 20\nLOG_INTERVAL = 5000\n\nconfig = BertConfig(decoder_start_token_id=tokenizer.pad_token_id)\nconfig.is_decoder = True\nmodel = BertLMHeadModel(config).from_pretrained(PRETRAINED_MODEL)\nmodel.resize_token_embeddings(len(tokenizer)) # to account for new special tokens\nmodel = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=LR)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:15:53.52894Z","iopub.execute_input":"2021-11-16T07:15:53.529395Z","iopub.status.idle":"2021-11-16T07:16:01.351893Z","shell.execute_reply.started":"2021-11-16T07:15:53.529362Z","shell.execute_reply":"2021-11-16T07:16:01.350927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVED_MODEL_PATH = \"qg_pretrained_bert_model_trained.pth\"\n\ndef train(epoch, best_val_loss):\n    model.train()\n    total_loss = 0.\n    for batch_index, batch in enumerate(train_loader):\n        data, target = batch\n        optimizer.zero_grad()\n        masked_labels = mask_label_padding(target['input_ids'])\n        output = model(\n            input_ids=data['input_ids'],\n            attention_mask=data['attention_mask'],\n            labels=masked_labels\n        )\n        loss = output[0]\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n            cur_loss = total_loss / LOG_INTERVAL\n            print('| epoch {:3d} | ' \n                  '{:5d}/{:5d} batches | '\n                  'loss {:5.2f}'.format(\n                    epoch, \n                    batch_index, len(train_loader), \n                    cur_loss))\n            \n            total_loss = 0\n\ndef evaluate(eval_model, data_loader):\n    eval_model.eval()\n    total_loss = 0.\n    with torch.no_grad():\n        for batch_index, batch in enumerate(data_loader):\n            data, target = batch\n            masked_labels = mask_label_padding(target['input_ids'])\n            output = eval_model(\n                input_ids=data['input_ids'],\n                attention_mask=data['attention_mask'],\n                labels=masked_labels\n            )\n            total_loss += output.loss\n    return total_loss / len(data_loader)\n\ndef mask_label_padding(labels):\n    MASK_ID = -100\n    labels = [\n           [(label if label != tokenizer.pad_token_id else MASK_ID) for label in labels_example] for labels_example in labels\n    ]\n    labels = torch.tensor(labels, device=device)\n    return labels\n\ndef save(path, epoch, model_state_dict, optimizer_state_dict, loss):\n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_state_dict,\n            'optimizer_state_dict': optimizer_state_dict,\n            'best_loss': loss,\n            }, path)\n\ndef load(path):\n    return torch.load(path)\n\ndef print_line():\n    LINE_WIDTH = 60","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:16:01.353147Z","iopub.execute_input":"2021-11-16T07:16:01.353389Z","iopub.status.idle":"2021-11-16T07:16:01.366454Z","shell.execute_reply.started":"2021-11-16T07:16:01.353359Z","shell.execute_reply":"2021-11-16T07:16:01.365094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\nbest_model = None\n\nval_loss = evaluate(model, valid_loader)\nprint_line()\nprint('| Before training | valid loss {:5.2f}'.format(\n    val_loss)\n)\nprint_line()\n\nfor epoch in range(1, EPOCHS + 1):\n\n    train(20, best_val_loss)\n    val_loss = evaluate(model, valid_loader)\n    print_line()\n    print('| end of epoch {:3d} | valid loss {:5.2f}'.format(\n        epoch,\n        val_loss)\n    )\n    print_line()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = model\n        save(\n             SAVED_MODEL_PATH,\n             epoch, \n             model.state_dict(), \n             optimizer.state_dict(), \n             best_val_loss\n        )\n        print(\"| Model saved.\")\n        print_line()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T07:16:01.367782Z","iopub.execute_input":"2021-11-16T07:16:01.368009Z","iopub.status.idle":"2021-11-16T07:20:49.808319Z","shell.execute_reply.started":"2021-11-16T07:16:01.367982Z","shell.execute_reply":"2021-11-16T07:20:49.806796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}