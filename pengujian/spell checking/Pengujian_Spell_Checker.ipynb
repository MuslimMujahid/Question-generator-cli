{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pengujian_Spell_Checker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKWssg8dmCof"
      },
      "source": [
        "!pip install unidecode --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBcOgvRzmDPG",
        "outputId": "187de7aa-d4b9-40ed-d24a-276eabcb37ee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqmzfs_zniGO",
        "outputId": "41cd3503-6cd1-4cdf-a092-0ead8fe7f4d2"
      },
      "source": [
        "!ls 'drive/MyDrive/NLP'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset  model\tPengujian_Spell_Checker.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyzwS-Xwn0B_"
      },
      "source": [
        "DIR = 'drive/MyDrive/NLP'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v71cg6DO8hIS"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras import optimizers, metrics, backend as K\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import unidecode\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(2434)\n",
        "\n",
        "SOS = '\\t' # start of sequence.\n",
        "EOS = '*' # end of sequence.\n",
        "CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
        "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'\n",
        "\n",
        "def truncated_acc(y_true, y_pred):\n",
        "    y_true = y_true[:, :16, :]\n",
        "    y_pred = y_pred[:, :16, :]\n",
        "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
        "    return K.mean(acc, axis=-1)\n",
        "    \n",
        "def truncated_loss(y_true, y_pred):\n",
        "    y_true = y_true[:, :16, :]\n",
        "    y_pred = y_pred[:, :16, :]\n",
        "    loss = K.categorical_crossentropy(\n",
        "        target=y_true, output=y_pred, from_logits=False)\n",
        "    return K.mean(loss, axis=-1)\n",
        "    \n",
        "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
        "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
        "                           name='encoder_data')\n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=True, return_state=False,\n",
        "                        name='encoder_lstm_1')\n",
        "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
        "    \n",
        "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
        "                        return_sequences=False, return_state=True,\n",
        "                        name='encoder_lstm_2')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    \n",
        "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
        "                           name='decoder_data')\n",
        "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
        "                        return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                         initial_state=encoder_states)\n",
        "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
        "                            name='decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    \n",
        "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                  outputs=decoder_outputs)\n",
        "    \n",
        "    adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
        "    \n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return model, encoder_model, decoder_model\n",
        "    \n",
        "class CharacterTable(object):\n",
        "    def __init__(self, chars):\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        self.size = len(self.chars)\n",
        "    \n",
        "    def encode(self, C, nb_rows):\n",
        "        x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char2index[c]] = 1.0\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            indices = x.argmax(axis=-1)\n",
        "        else:\n",
        "            indices = x\n",
        "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
        "        return indices, chars\n",
        "\n",
        "    def sample_multinomial(self, preds, temperature=1.0):\n",
        "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probs = np.random.multinomial(1, preds, 1)\n",
        "        index = np.argmax(probs)\n",
        "        char  = self.index2char[index]\n",
        "        return index, char\n",
        "\n",
        "def split_text(data_path, input_path):\n",
        "    file_path = os.path.join(data_path, input_path)\n",
        "    with open(file_path, \"r\", errors='replace') as f:\n",
        "        data = f.read().split('\\n')\n",
        "        data = np.array(data)\n",
        "        train, test = train_test_split(data,test_size=0.2)\n",
        "        train_text = '\\n'.join([i for i in train[1:]])\n",
        "        test_text = '\\n'.join([i for i in test[1:]])\n",
        "        return train_text, test_text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
        "              for token in re.split(\"[-\\n ]\", text)]\n",
        "    return tokens\n",
        "\n",
        "def add_spelling_errors(token, error_rate):\n",
        "    assert(0.0 <= error_rate < 1.0)\n",
        "    if len(token) < 3:\n",
        "        return token\n",
        "    rand = np.random.rand()\n",
        "    prob = error_rate / 4.0\n",
        "    if rand < prob:\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index + 1:]\n",
        "    elif prob * 2 < rand < prob * 3:\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
        "                + token[random_char_index:]\n",
        "    elif prob < rand < prob * 2:\n",
        "        random_char_index = np.random.randint(len(token))\n",
        "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
        "    elif prob * 3 < rand < prob * 4:\n",
        "        random_char_index = np.random.randint(len(token) - 1)\n",
        "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
        "                + token[random_char_index] + token[random_char_index + 2:]\n",
        "    else:\n",
        "        pass\n",
        "    return token\n",
        "\n",
        "def transform(tokens, maxlen, error_rate=0.5, shuffle=True):\n",
        "    if shuffle:\n",
        "        print('Shuffling data.')\n",
        "        np.random.shuffle(tokens)\n",
        "    encoder_tokens = []\n",
        "    decoder_tokens = []\n",
        "    target_tokens = []\n",
        "    for token in tokens:\n",
        "        encoder = add_spelling_errors(token, error_rate=error_rate)\n",
        "        encoder += EOS * (maxlen - len(encoder))\n",
        "        encoder_tokens.append(encoder)\n",
        "    \n",
        "        decoder = SOS + token\n",
        "        decoder += EOS * (maxlen - len(decoder))\n",
        "        decoder_tokens.append(decoder)\n",
        "    \n",
        "        target = decoder[1:]\n",
        "        target += EOS * (maxlen - len(target))\n",
        "        target_tokens.append(target)\n",
        "        \n",
        "        assert(len(encoder) == len(decoder) == len(target))\n",
        "    return encoder_tokens, decoder_tokens, target_tokens\n",
        "\n",
        "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
        "    def generate(tokens, reverse):\n",
        "        while(True):\n",
        "            for token in tokens:\n",
        "                if reverse:\n",
        "                    token = token[::-1]\n",
        "                yield token\n",
        "    \n",
        "    token_iterator = generate(tokens, reverse)\n",
        "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
        "                          dtype=np.float32)\n",
        "    while(True):\n",
        "        for i in range(batch_size):\n",
        "            token = next(token_iterator)\n",
        "            data_batch[i] = ctable.encode(token, maxlen)\n",
        "        yield data_batch\n",
        "\n",
        "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
        "                     maxlen, reverse, encoder_model, decoder_model,\n",
        "                     nb_examples, sample_mode='argmax', random=True):\n",
        "    input_tokens = []\n",
        "    target_tokens = []\n",
        "    \n",
        "    if random:\n",
        "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
        "    else:\n",
        "        indices = range(nb_examples)\n",
        "        \n",
        "    for index in indices:\n",
        "        input_tokens.append(inputs[index])\n",
        "        target_tokens.append(targets[index])\n",
        "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
        "                            nb_examples, reverse)\n",
        "    input_sequences = next(input_sequences)\n",
        "    \n",
        "    print(\"Input sequence shape:\")\n",
        "    print(input_sequences.shape)\n",
        "\n",
        "    states_value = encoder_model.predict(input_sequences)\n",
        "    \n",
        "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "\n",
        "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
        "\n",
        "    decoded_tokens = [''] * nb_examples\n",
        "    for _ in range(maxlen):\n",
        "        char_probs, h, c = decoder_model.predict(\n",
        "            [target_sequences] + states_value)\n",
        "\n",
        "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
        "\n",
        "        sampled_chars = []\n",
        "        for i in range(nb_examples):\n",
        "            if sample_mode == 'argmax':\n",
        "                next_index, next_char = target_ctable.decode(\n",
        "                    char_probs[i], calc_argmax=True)\n",
        "            elif sample_mode == 'multinomial':\n",
        "                next_index, next_char = target_ctable.sample_multinomial(\n",
        "                    char_probs[i], temperature=0.5)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
        "            decoded_tokens[i] += next_char\n",
        "            sampled_chars.append(next_char) \n",
        "            # Update target sequence with index of next character.\n",
        "            target_sequences[i, 0, next_index] = 1.0\n",
        "\n",
        "        stop_char = set(sampled_chars)\n",
        "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
        "            break\n",
        "            \n",
        "        # Update states.\n",
        "        states_value = [h, c]\n",
        "    \n",
        "    # Sampling finished.\n",
        "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in input_tokens]\n",
        "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in target_tokens]\n",
        "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
        "                      for token in decoded_tokens]\n",
        "    return input_tokens, target_tokens, decoded_tokens\n",
        "\n",
        "def restore_model(model_path, hidden_size):\n",
        "    model = load_model(model_path, custom_objects={\n",
        "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
        "    \n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
        "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
        "    \n",
        "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
        "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.get_layer('decoder_lstm')\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_softmax = model.get_layer('decoder_softmax')\n",
        "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
        "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "                          outputs=[decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "def spell_correction(test_text):\n",
        "    text,_ = split_text(f'{DIR}/dataset/', 'movie_lines_preprocess.txt')\n",
        "    vocab = tokenize(text)\n",
        "    vocab = list(filter(None, set(vocab)))\n",
        "    maxlen = max([len(token) for token in vocab]) + 2\n",
        "    train_encoder, train_decoder, train_target = transform(\n",
        "        vocab, maxlen, error_rate=0.6, shuffle=False)\n",
        "\n",
        "    tokens = tokenize(test_text)\n",
        "    tokens = list(filter(None, tokens))\n",
        "    nb_tokens = len(tokens)\n",
        "    final_tokens, _, target_tokens = transform(\n",
        "        tokens, maxlen, error_rate=0.6, shuffle=False)\n",
        "\n",
        "    input_chars = set(' '.join(train_encoder))\n",
        "    target_chars = set(' '.join(train_decoder))\n",
        "    input_ctable = CharacterTable(input_chars)\n",
        "    target_ctable = CharacterTable(target_chars)\n",
        "\n",
        "    encoder_model, decoder_model = restore_model(f'{DIR}/model/seq2seq_spellcorrection.h5', 512)\n",
        "\n",
        "    input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
        "        final_tokens, target_tokens, input_ctable, target_ctable,\n",
        "        maxlen, True, encoder_model, decoder_model, nb_tokens,\n",
        "        sample_mode='argmax', random=False)\n",
        "    total_input = ' '.join([token for token in input_tokens])\n",
        "    total_decoded = ' '.join([token for token in decoded_tokens])\n",
        "    total_target = ' '.join([token for token in target_tokens])\n",
        "    print('-')\n",
        "    print('Input sentence:  ', total_input)\n",
        "    print('-')\n",
        "    print('Input sentence:  ', total_decoded)\n",
        "    print('-')\n",
        "    print('Input sentence:  ', total_target)\n",
        "\n",
        "    return total_input,total_decoded, total_target"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK171U2fnnN-"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZZoyZJToDC3"
      },
      "source": [
        "text = \"We live on a placid island of ignorance in the midst of black seas of the infinity, and it was not meant that we should voyage far.\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cPZzw5ixtnG"
      },
      "source": [
        "### Model Encoder-Decoder Seq2Seq Arsitektur LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjo9R-c0FpeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fd023e-9322-4587-d0c4-1a6be02ec38f"
      },
      "source": [
        "input_tokens, target_tokens, decoded_tokens = spell_correction(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence shape:\n",
            "(27, 40, 56)\n",
            "-\n",
            "Input sentence:   We live on a plaid island of ignoracne in teh mids of black seas of thG infinity and it was  ot meant tht we shoAuld vyage fiar\n",
            "-\n",
            "Input sentence:   We live on a plaid island of ingorance in teh mids of black seas of thG infinity and it was oot meant thut we should vaye fiar\n",
            "-\n",
            "Input sentence:   We live on a placid island of ignorance in the midst of black seas of the infinity and it was not meant that we should voyage far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlOHTMAJKolV"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}